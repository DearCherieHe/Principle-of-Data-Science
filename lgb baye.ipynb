{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_X=pd.read_csv('/Users/cheriehe/scaled_test_X.csv')\n",
    "scaled_test_X.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "scaled_train_X=pd.read_csv('/Users/cheriehe/scaled_train_X.csv')\n",
    "train_y=pd.read_csv('/Users/cheriehe/train_y.csv')\n",
    "scaled_train_X.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "train_y.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.430797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.391499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.353196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.313798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.274400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_to_failure\n",
       "0         1.430797\n",
       "1         1.391499\n",
       "2         1.353196\n",
       "3         1.313798\n",
       "4         1.274400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=train_y['time_to_failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_parameter_opt_lgb(X, y, init_round=2, opt_round=8, n_folds=5, random_seed=6, n_estimators=20000, learning_rate=0.001, output_process=True):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y,free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(num_leaves, colsample_bytree, subsample, max_depth, reg_lambda, reg_alpha, min_split_gain, min_child_weight, \n",
    "                min_child_sample, max_bin, subsample_freq):\n",
    "        params = {'objective':'regression','boosting_type': 'gbdt','nthread': -1, 'verbose': -1,\\\n",
    "                  'num_boost_round': n_estimators, 'learning_rate':learning_rate, \\\n",
    "                  'early_stopping_round':500}\n",
    "        params['subsample_freq']=int(round(subsample_freq))\n",
    "        params['min_child_sample']=int(round(min_child_sample))\n",
    "        params['max_bin']=int(round(max_bin))\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['reg_lambda'] = max(reg_lambda, 0)\n",
    "        params['reg_alpha'] = max(reg_alpha, 0)\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=False, verbose_eval=1000, metrics=['rmse'])\n",
    "        return -1.0 * np.mean(cv_result['rmse-mean'])\n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (20, 80),\n",
    "                                            'colsample_bytree': (0.6, 1.0),\n",
    "                                            'subsample': (0.6, 1.0),\n",
    "                                            'max_depth': (-1, 8),\n",
    "                                            'reg_lambda': (0, 1),\n",
    "                                            'reg_alpha': (0, 1),\n",
    "                                            'min_child_sample':(10,50),\n",
    "                                            'max_bin':(180,500),\n",
    "                                            'subsample_freq':(1,10),\n",
    "                                            'min_split_gain': (0.1, 0.8),\n",
    "                                            'min_child_weight': (3, 20)})\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "\n",
    "opt_params = bayes_parameter_opt_lgb(scaled_train_X, train_y, init_round=2, opt_round=8, n_folds=5, random_seed=6, n_estimators=20000, learning_rate=0.001,output_process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "train_columns = scaled_train_X.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    params = {'boosting_type': 'gbdt',\n",
    "              'objective': 'regression',\n",
    "              'metric':'mae',\n",
    "              'learning_rate': 0.001,\n",
    "              'num_leaves': 53, \n",
    "              'max_depth': 2,  \n",
    "              'min_child_samples': 37,  \n",
    "              'max_bin': 443,  \n",
    "              'subsample': 0.6815,  \n",
    "              'subsample_freq': 1,  \n",
    "              'colsample_bytree':0.8397,  \n",
    "              'min_split_gain': 0.2689 ,\n",
    "              'min_child_weight': 11.3791,\n",
    "              'reg_lambda':0.4544,\n",
    "              'reg_alpha': 0.0361,\n",
    "              'nthread': 8,\n",
    "              'verbose': -1,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.zeros(len(scaled_train_X))\n",
    "predictions = np.zeros(len(scaled_test_X))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "#run model\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(scaled_train_X,train_y.values)):\n",
    "    strLog = \"fold {}\".format(fold_)\n",
    "    print(strLog)\n",
    "    \n",
    "    X_tr, X_val = scaled_train_X.iloc[trn_idx], scaled_train_X.iloc[val_idx]\n",
    "    y_tr, y_val = train_y.iloc[trn_idx], train_y.iloc[val_idx]\n",
    "    dtrain = lgb.Dataset(data=X_tr, \n",
    "                         label=y_tr,\n",
    "                         free_raw_data=False)\n",
    "    dvalid = lgb.Dataset(data=X_val,\n",
    "                         label=y_val,\n",
    "                         free_raw_data=False)\n",
    "    model = lgb.train(params, \n",
    "                    dtrain, \n",
    "                    valid_sets=[dtrain, dvalid], \n",
    "                    valid_names=['train','valid'],\n",
    "                    num_boost_round=20000,\n",
    "                    early_stopping_rounds=500,\n",
    "                    verbose_eval=1000)\n",
    "    oof[val_idx] = model.predict(X_val)\n",
    "    predictions += model.predict(scaled_test_X) / folds.n_splits\n",
    "    #feature importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = train_columns\n",
    "    fold_importance_df[\"importance\"] = model.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    del model, dtrain, dvalid\n",
    "    gc.collect()\n",
    "print('Full mae %.6f' % mean_absolute_error(train_y, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).index\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv', index_col='seg_id')\n",
    "submission.time_to_failure = predictions\n",
    "submission.to_csv('submission_lgb2.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cols = list( feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  feature selection: 120, 100, 80, 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking less columns \n",
    "scaled_train_X1 = scaled_train_X[top_cols]\n",
    "scaled_test_X1 = scaled_test_X[top_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_parameter_opt_lgb(X, y, init_round=2, opt_round=8, n_folds=10, random_seed=6, n_estimators=20000, learning_rate=0.001, output_process=True):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y,free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(num_leaves, colsample_bytree, subsample, max_depth, reg_lambda, reg_alpha, min_split_gain, min_child_weight, \n",
    "                min_child_sample, max_bin, subsample_freq):\n",
    "        params = {'objective':'regression','boosting_type': 'gbdt','nthread': -1, 'verbose': -1,\\\n",
    "                  'num_boost_round': n_estimators, 'learning_rate':learning_rate, \\\n",
    "                  'early_stopping_round':500}\n",
    "        params['subsample_freq']=int(round(subsample_freq))\n",
    "        params['min_child_sample']=int(round(min_child_sample))\n",
    "        params['max_bin']=int(round(max_bin))\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['reg_lambda'] = max(reg_lambda, 0)\n",
    "        params['reg_alpha'] = max(reg_alpha, 0)\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=False, verbose_eval=1000, metrics=['rmse'])\n",
    "        return -1.0 * np.mean(cv_result['rmse-mean'])\n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (20, 80),\n",
    "                                            'colsample_bytree': (0.6, 1.0),\n",
    "                                            'subsample': (0.6, 1.0),\n",
    "                                            'max_depth': (-1, 8),\n",
    "                                            'reg_lambda': (0, 1),\n",
    "                                            'reg_alpha': (0, 1),\n",
    "                                            'min_child_sample':(10,50),\n",
    "                                            'max_bin':(180,500),\n",
    "                                            'subsample_freq':(1,10),\n",
    "                                            'min_split_gain': (0.1, 0.8),\n",
    "                                            'min_child_weight': (3, 20)})\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "\n",
    "opt_params = bayes_parameter_opt_lgb(scaled_train_X1, train_y, init_round=2, opt_round=8, n_folds=10, random_seed=6, n_estimators=20000, learning_rate=0.001,output_process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "train_columns = scaled_train_X1.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    params = {'boosting_type': 'gbdt',\n",
    "              'objective': 'regression',\n",
    "              'metric':'mae',\n",
    "              'learning_rate': 0.001,\n",
    "              'num_leaves': 47, \n",
    "              'max_depth': 2,  \n",
    "              'min_child_samples': 32,  \n",
    "              'max_bin': 383,  \n",
    "              'subsample': 0.6911,  \n",
    "              'subsample_freq': 8,  \n",
    "              'colsample_bytree':.6596,  \n",
    "              'min_split_gain': 0.690 ,\n",
    "              'min_child_weight': 15.6388,\n",
    "              'reg_lambda':0.4518,\n",
    "              'reg_alpha': 0.9155,\n",
    "              'nthread': 8,\n",
    "              'verbose': -1,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.zeros(len(scaled_train_X1))\n",
    "predictions = np.zeros(len(scaled_test_X1))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "#run model\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(scaled_train_X1,train_y.values)):\n",
    "    strLog = \"fold {}\".format(fold_)\n",
    "    print(strLog)\n",
    "    \n",
    "    X_tr, X_val = scaled_train_X1.iloc[trn_idx], scaled_train_X1.iloc[val_idx]\n",
    "    y_tr, y_val = train_y.iloc[trn_idx], train_y.iloc[val_idx]\n",
    "    dtrain = lgb.Dataset(data=X_tr, \n",
    "                         label=y_tr,\n",
    "                         free_raw_data=False)\n",
    "    dvalid = lgb.Dataset(data=X_val,\n",
    "                         label=y_val,\n",
    "                         free_raw_data=False)\n",
    "    model = lgb.train(params, \n",
    "                    dtrain, \n",
    "                    valid_sets=[dtrain, dvalid], \n",
    "                    valid_names=['train','valid'],\n",
    "                    num_boost_round=20000,\n",
    "                    early_stopping_rounds=500,\n",
    "                    verbose_eval=1000)\n",
    "    oof[val_idx] = model.predict(X_val)\n",
    "    predictions += model.predict(scaled_test_X1) / folds.n_splits\n",
    "    #feature importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = train_columns\n",
    "    fold_importance_df[\"importance\"] = model.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    del model, dtrain, dvalid\n",
    "    gc.collect()\n",
    "print('Full mae %.6f' % mean_absolute_error(train_y, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv', index_col='seg_id')\n",
    "submission.time_to_failure = predictions\n",
    "submission.to_csv('submission_lgb5.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).index\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
